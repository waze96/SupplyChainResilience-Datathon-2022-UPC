{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import xgboost as xgb\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Functions\n",
    "PROPHET = 'Prophet'\n",
    "XGBOOST = 'XGBoost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aaaaaa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m aaaaaa\n",
      "\u001b[1;31mNameError\u001b[0m: name 'aaaaaa' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalHashError",
     "evalue": "module '__main__' has no attribute '__file__'\n\nWhile caching the body of `convert_df()`, Streamlit encountered an\nobject of type `builtins.function`, which it does not know how to hash.\n\n**In this specific case, it's very likely you found a Streamlit bug so please\n[file a bug report here.]\n(https://github.com/streamlit/streamlit/issues/new/choose)**\n\nIn the meantime, you can try bypassing this error by registering a custom\nhash function via the `hash_funcs` keyword in @st.cache(). For example:\n\n```\n@st.cache(hash_funcs={builtins.function: my_hash_func})\ndef my_func(...):\n    ...\n```\n\nIf you don't know where the object of type `builtins.function` is coming\nfrom, try looking at the hash chain below for an object that you do recognize,\nthen pass that to `hash_funcs` instead:\n\n```\nObject of type builtins.function: <function convert_df at 0x00000215EC536940>\n```\n\nPlease see the `hash_funcs` [documentation]\n(https://docs.streamlit.io/library/advanced-features/caching#the-hash_funcs-parameter)\nfor more details.\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\hashing.py:361\u001b[0m, in \u001b[0;36m_CodeHasher.to_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    360\u001b[0m     \u001b[39m# Hash the input\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (tname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_to_bytes(obj, context))\n\u001b[0;32m    363\u001b[0m     \u001b[39m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[39;00m\n\u001b[0;32m    364\u001b[0m     \u001b[39m# call to_bytes inside _to_bytes things get double-counted.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\hashing.py:627\u001b[0m, in \u001b[0;36m_CodeHasher._to_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39massert\u001b[39;00m code \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_file_should_be_hashed(code\u001b[39m.\u001b[39;49mco_filename):\n\u001b[0;32m    628\u001b[0m     context \u001b[39m=\u001b[39m _get_context(obj)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\hashing.py:403\u001b[0m, in \u001b[0;36m_CodeHasher._file_should_be_hashed\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[39mreturn\u001b[39;00m file_util\u001b[39m.\u001b[39mfile_is_in_folder_glob(\n\u001b[1;32m--> 403\u001b[0m     filepath, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_main_script_directory()\n\u001b[0;32m    404\u001b[0m ) \u001b[39mor\u001b[39;00m file_util\u001b[39m.\u001b[39mfile_in_pythonpath(filepath)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\hashing.py:715\u001b[0m, in \u001b[0;36m_CodeHasher._get_main_script_directory\u001b[1;34m()\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[39m# This works because we set __main__.__file__ to the\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# script path in ScriptRunner.\u001b[39;00m\n\u001b[1;32m--> 715\u001b[0m abs_main_path \u001b[39m=\u001b[39m pathlib\u001b[39m.\u001b[39mPath(__main__\u001b[39m.\u001b[39;49m\u001b[39m__file__\u001b[39;49m)\u001b[39m.\u001b[39mresolve()\n\u001b[0;32m    716\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mstr\u001b[39m(abs_main_path\u001b[39m.\u001b[39mparent)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module '__main__' has no attribute '__file__'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInternalHashError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 111\u001b[0m\n\u001b[0;32m    108\u001b[0m         st\u001b[39m.\u001b[39mwrite(results)\n\u001b[0;32m    110\u001b[0m     \u001b[39m# DOWNLOAD RESULTS\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m     csv \u001b[39m=\u001b[39m convert_df(df_uploaded_file)\n\u001b[0;32m    112\u001b[0m     st\u001b[39m.\u001b[39mdownload_button(\n\u001b[0;32m    113\u001b[0m         label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDescarga los resultados\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    114\u001b[0m         data\u001b[39m=\u001b[39mcsv,\n\u001b[0;32m    115\u001b[0m         file_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mresultados.csv\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    116\u001b[0m         mime\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtext/csv\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    117\u001b[0m     )\n\u001b[0;32m    119\u001b[0m \u001b[39mwith\u001b[39;00m tab2:\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\caching.py:623\u001b[0m, in \u001b[0;36mcache.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[39mif\u001b[39;00m show_spinner:\n\u001b[0;32m    622\u001b[0m     \u001b[39mwith\u001b[39;00m spinner(message):\n\u001b[1;32m--> 623\u001b[0m         \u001b[39mreturn\u001b[39;00m get_or_create_cached_value()\n\u001b[0;32m    624\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    625\u001b[0m     \u001b[39mreturn\u001b[39;00m get_or_create_cached_value()\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\caching.py:548\u001b[0m, in \u001b[0;36mcache.<locals>.wrapped_func.<locals>.get_or_create_cached_value\u001b[1;34m()\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[39mnonlocal\u001b[39;00m cache_key\n\u001b[0;32m    542\u001b[0m \u001b[39mif\u001b[39;00m cache_key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    543\u001b[0m     \u001b[39m# Delay generating the cache key until the first call.\u001b[39;00m\n\u001b[0;32m    544\u001b[0m     \u001b[39m# This way we can see values of globals, including functions\u001b[39;00m\n\u001b[0;32m    545\u001b[0m     \u001b[39m# defined after this one.\u001b[39;00m\n\u001b[0;32m    546\u001b[0m     \u001b[39m# If we generated the key earlier we would only hash those\u001b[39;00m\n\u001b[0;32m    547\u001b[0m     \u001b[39m# globals by name, and miss changes in their code or value.\u001b[39;00m\n\u001b[1;32m--> 548\u001b[0m     cache_key \u001b[39m=\u001b[39m _hash_func(non_optional_func, hash_funcs)\n\u001b[0;32m    550\u001b[0m \u001b[39m# First, get the cache that's attached to this function.\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[39m# This cache's key is generated (above) from the function's code.\u001b[39;00m\n\u001b[0;32m    552\u001b[0m mem_cache \u001b[39m=\u001b[39m _mem_caches\u001b[39m.\u001b[39mget_cache(cache_key, max_entries, ttl)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\caching.py:674\u001b[0m, in \u001b[0;36m_hash_func\u001b[1;34m(func, hash_funcs)\u001b[0m\n\u001b[0;32m    663\u001b[0m update_hash(\n\u001b[0;32m    664\u001b[0m     (func\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m, func\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m),\n\u001b[0;32m    665\u001b[0m     hasher\u001b[39m=\u001b[39mfunc_hasher,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    668\u001b[0m     hash_source\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m    669\u001b[0m )\n\u001b[0;32m    671\u001b[0m \u001b[39m# Include the function's body in the hash. We *do* pass hash_funcs here,\u001b[39;00m\n\u001b[0;32m    672\u001b[0m \u001b[39m# because this step will be hashing any objects referenced in the function\u001b[39;00m\n\u001b[0;32m    673\u001b[0m \u001b[39m# body.\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m update_hash(\n\u001b[0;32m    675\u001b[0m     func,\n\u001b[0;32m    676\u001b[0m     hasher\u001b[39m=\u001b[39;49mfunc_hasher,\n\u001b[0;32m    677\u001b[0m     hash_funcs\u001b[39m=\u001b[39;49mhash_funcs,\n\u001b[0;32m    678\u001b[0m     hash_reason\u001b[39m=\u001b[39;49mHashReason\u001b[39m.\u001b[39;49mCACHING_FUNC_BODY,\n\u001b[0;32m    679\u001b[0m     hash_source\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    680\u001b[0m )\n\u001b[0;32m    681\u001b[0m cache_key \u001b[39m=\u001b[39m func_hasher\u001b[39m.\u001b[39mhexdigest()\n\u001b[0;32m    682\u001b[0m _LOGGER\u001b[39m.\u001b[39mdebug(\n\u001b[0;32m    683\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmem_cache key for \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, func\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m, func\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m, cache_key\n\u001b[0;32m    684\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\hashing.py:109\u001b[0m, in \u001b[0;36mupdate_hash\u001b[1;34m(val, hasher, hash_reason, hash_source, context, hash_funcs)\u001b[0m\n\u001b[0;32m    106\u001b[0m hash_stacks\u001b[39m.\u001b[39mcurrent\u001b[39m.\u001b[39mhash_source \u001b[39m=\u001b[39m hash_source\n\u001b[0;32m    108\u001b[0m ch \u001b[39m=\u001b[39m _CodeHasher(hash_funcs)\n\u001b[1;32m--> 109\u001b[0m ch\u001b[39m.\u001b[39;49mupdate(hasher, val, context)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\hashing.py:386\u001b[0m, in \u001b[0;36m_CodeHasher.update\u001b[1;34m(self, hasher, obj, context)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(\u001b[39mself\u001b[39m, hasher, obj: Any, context: Optional[Context] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     \u001b[39m\"\"\"Update the provided hasher with the hash of an object.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto_bytes(obj, context)\n\u001b[0;32m    387\u001b[0m     hasher\u001b[39m.\u001b[39mupdate(b)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\hashing.py:375\u001b[0m, in \u001b[0;36m_CodeHasher.to_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[39mraise\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 375\u001b[0m     \u001b[39mraise\u001b[39;00m InternalHashError(e, obj)\n\u001b[0;32m    377\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m     \u001b[39m# In case an UnhashableTypeError (or other) error is thrown, clean up the\u001b[39;00m\n\u001b[0;32m    379\u001b[0m     \u001b[39m# stack so we don't get false positives in future hashing calls\u001b[39;00m\n\u001b[0;32m    380\u001b[0m     hash_stacks\u001b[39m.\u001b[39mcurrent\u001b[39m.\u001b[39mpop()\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\hashing.py:361\u001b[0m, in \u001b[0;36m_CodeHasher.to_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    357\u001b[0m hash_stacks\u001b[39m.\u001b[39mcurrent\u001b[39m.\u001b[39mpush(obj)\n\u001b[0;32m    359\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    360\u001b[0m     \u001b[39m# Hash the input\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (tname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_to_bytes(obj, context))\n\u001b[0;32m    363\u001b[0m     \u001b[39m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[39;00m\n\u001b[0;32m    364\u001b[0m     \u001b[39m# call to_bytes inside _to_bytes things get double-counted.\u001b[39;00m\n\u001b[0;32m    365\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mgetsizeof(b)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\hashing.py:627\u001b[0m, in \u001b[0;36m_CodeHasher._to_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    625\u001b[0m code \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__code__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    626\u001b[0m \u001b[39massert\u001b[39;00m code \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_file_should_be_hashed(code\u001b[39m.\u001b[39;49mco_filename):\n\u001b[0;32m    628\u001b[0m     context \u001b[39m=\u001b[39m _get_context(obj)\n\u001b[0;32m    629\u001b[0m     defaults \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__defaults__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\hashing.py:403\u001b[0m, in \u001b[0;36m_CodeHasher._file_should_be_hashed\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[39mif\u001b[39;00m file_is_blacklisted:\n\u001b[0;32m    401\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[39mreturn\u001b[39;00m file_util\u001b[39m.\u001b[39mfile_is_in_folder_glob(\n\u001b[1;32m--> 403\u001b[0m     filepath, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_main_script_directory()\n\u001b[0;32m    404\u001b[0m ) \u001b[39mor\u001b[39;00m file_util\u001b[39m.\u001b[39mfile_in_pythonpath(filepath)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\hashing.py:715\u001b[0m, in \u001b[0;36m_CodeHasher._get_main_script_directory\u001b[1;34m()\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39m__main__\u001b[39;00m\n\u001b[0;32m    713\u001b[0m \u001b[39m# This works because we set __main__.__file__ to the\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# script path in ScriptRunner.\u001b[39;00m\n\u001b[1;32m--> 715\u001b[0m abs_main_path \u001b[39m=\u001b[39m pathlib\u001b[39m.\u001b[39mPath(__main__\u001b[39m.\u001b[39;49m\u001b[39m__file__\u001b[39;49m)\u001b[39m.\u001b[39mresolve()\n\u001b[0;32m    716\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mstr\u001b[39m(abs_main_path\u001b[39m.\u001b[39mparent)\n",
      "\u001b[1;31mInternalHashError\u001b[0m: module '__main__' has no attribute '__file__'\n\nWhile caching the body of `convert_df()`, Streamlit encountered an\nobject of type `builtins.function`, which it does not know how to hash.\n\n**In this specific case, it's very likely you found a Streamlit bug so please\n[file a bug report here.]\n(https://github.com/streamlit/streamlit/issues/new/choose)**\n\nIn the meantime, you can try bypassing this error by registering a custom\nhash function via the `hash_funcs` keyword in @st.cache(). For example:\n\n```\n@st.cache(hash_funcs={builtins.function: my_hash_func})\ndef my_func(...):\n    ...\n```\n\nIf you don't know where the object of type `builtins.function` is coming\nfrom, try looking at the hash chain below for an object that you do recognize,\nthen pass that to `hash_funcs` instead:\n\n```\nObject of type builtins.function: <function convert_df at 0x00000215EC536940>\n```\n\nPlease see the `hash_funcs` [documentation]\n(https://docs.streamlit.io/library/advanced-features/caching#the-hash_funcs-parameter)\nfor more details.\n            "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    def f_xgboost(input, date_column: str, objective_column: str):\n",
    "        data = pd.DataFrame(input)\n",
    "\n",
    "        X, y = np.vstack(data.index.values), np.vstack(data[objective_column].values)\n",
    "\n",
    "        data_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "        xg_reg = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1, max_depth=15, alpha=10,\n",
    "                                  n_estimators=1000)\n",
    "\n",
    "        xg_reg.fit(X_train, y_train)\n",
    "\n",
    "        preds = xg_reg.predict(X_test)\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "\n",
    "\n",
    "        df_prediction = pd.DataFrame({\n",
    "            date_column: input.loc[X_test.reshape(len(X_test)), date_column],\n",
    "            'Actual_temp': y_test.reshape(len(y_test)),\n",
    "            'Pred_temp': preds\n",
    "        })\n",
    "        st.write(\"RMSE: %f\" % rmse)\n",
    "\n",
    "        params = {\"objective\": \"reg:squarederror\", 'colsample_bytree': 0.3, 'learning_rate': 0.1, 'max_depth': 15, 'alpha': 10}\n",
    "\n",
    "        cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n",
    "                            num_boost_round=50, early_stopping_rounds=10, metrics=\"rmse\", as_pandas=True)\n",
    "        return cv_results, df_prediction\n",
    "        # return None\n",
    "\n",
    "\n",
    "    def f_prophet(input, date_column: str, objective_column: str):\n",
    "        data = pd.DataFrame({\n",
    "            'ds': input[date_column].values,\n",
    "            'y': input[objective_column].values\n",
    "        })\n",
    "        n_split = round(len(data)*0.8)\n",
    "\n",
    "        train_df = data[:n_split]\n",
    "        test_df = data[n_split:]\n",
    "\n",
    "        p_model = Prophet()\n",
    "        p_model.fit(train_df)\n",
    "\n",
    "        prediction = p_model.predict(test_df[['ds']])\n",
    "        prediction = prediction[['ds', 'yhat']]\n",
    "\n",
    "        df_prediction = pd.DataFrame({\n",
    "            date_column: test_df['ds'].values,\n",
    "            'Actual_temp': test_df['y'].values,\n",
    "            'Pred_temp': prediction['yhat'].values\n",
    "        })\n",
    "        cv_results = None\n",
    "        return cv_results, df_prediction\n",
    "\n",
    "    @st.cache\n",
    "    def convert_df(df):\n",
    "\n",
    "        return df.to_csv().encode('utf-8')\n",
    "\n",
    "\n",
    "    # Web structure\n",
    "\n",
    "\n",
    "    st.markdown(\n",
    "        \"<img alt='FCC' src='https://profesoresim2019.upc.edu/wp/wp-content/uploads/2018/09/logo_UPC.png' width='57px' height='57px' style='text-align: center; float: left'></img>\"\n",
    "        \"<img alt='T2C' src='data:image/png;base64,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' width='185px' height='57px' style='text-align: center; float: right'></img>\"\n",
    "        \"<style>.block-container{ max-width: 55rem;}</style>\", unsafe_allow_html=True)\n",
    "\n",
    "    with st.sidebar:\n",
    "        # Can be used wherever a \"file-like\" object is accepted:\n",
    "        uploaded_file = st.file_uploader(\"Carga un nuevo CSV\", type=[\"csv\",\"xlsx\"])\n",
    "        file_separator = st.radio('Which is the CSV column separator?', options=[';',','])\n",
    "        df_uploaded_file = pd.DataFrame()\n",
    "        if uploaded_file is not None:\n",
    "            df_uploaded_file = pd.read_csv(uploaded_file, sep=file_separator)\n",
    "\n",
    "    tab1, tab2 = st.tabs([\"Resultados\", \"Mapa\"])\n",
    "\n",
    "    with tab1:\n",
    "        st.subheader(\"Resultados\")\n",
    "\n",
    "        # SHOW UPLOAD\n",
    "        if not df_uploaded_file.empty:\n",
    "            st.write(df_uploaded_file)\n",
    "            desired_columns = st.multiselect('Select desired columns', options=df_uploaded_file.columns)\n",
    "            date_column = st.radio('Which is the date variable', options=desired_columns)\n",
    "            objective_column = st.radio('Which is the objective variable', options=desired_columns)\n",
    "\n",
    "        # BUTTON\n",
    "        results = None\n",
    "        model = st.radio('Which model to use?', options=[PROPHET, XGBOOST])\n",
    "        if st.button(\"Execute\"):\n",
    "            df_to_predict = df_uploaded_file[desired_columns]\n",
    "            if model == PROPHET:\n",
    "                results, df_prediction = f_prophet(input=df_to_predict, date_column=date_column, objective_column=objective_column)\n",
    "            if model == XGBOOST:\n",
    "                results, df_prediction = f_xgboost(input=df_to_predict, date_column=date_column, objective_column=objective_column)\n",
    "            df_prediction[date_column] = pd.to_datetime(df_prediction[date_column], format='%d/%m/%Y')\n",
    "            st.line_chart(df_prediction, x=date_column)\n",
    "\n",
    "        # SHOW RESULTS\n",
    "        if results:\n",
    "            # results = f_xgboost(input=df_uploaded_file)\n",
    "            st.write(results)\n",
    "\n",
    "        # DOWNLOAD RESULTS\n",
    "        csv = convert_df(df_uploaded_file)\n",
    "        st.download_button(\n",
    "            label=\"Descarga los resultados\",\n",
    "            data=csv,\n",
    "            file_name='resultados.csv',\n",
    "            mime='text/csv',\n",
    "        )\n",
    "\n",
    "    with tab2:\n",
    "        st.subheader(\"Mapa\")\n",
    "        with st.spinner('Cargando el mapa'):\n",
    "            df_coordinates = pd.DataFrame(\n",
    "                data={'lat': [41.38945007324219, 36.72016], 'lon': [2.1317803859710693, -4.42034]})\n",
    "            st.map(df_coordinates)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
